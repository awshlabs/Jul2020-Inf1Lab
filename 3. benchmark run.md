# Lab 3: Compile and launch a load test run on Inf1 Instance.

**Please complete Lab 2 and clean up by following Lab 2's last step. If using DLAMI Conda environment, please update to [latest Neuron software](https://github.com/aws/aws-neuron-sdk/blob/master/release-notes/dlami-release-notes.md) for this lab.**

This lab shows an example load testing using FP16 model derived from Keras ResNet50 model and compiled to Inferentia with experimental performance flags. .

## Lab 3 Section 1: Compile 
> If you were previously disconnected.  Reconnect to host, and run:

```bash
source test_env_p36/bin/activate
```

**3.1.1** Download and unpack the ResNet50 performance package:

```bash
wget https://reinventinf1.s3.amazonaws.com/keras_fp16_benchmarking_db.tgz
```
```bash
tar -xzf keras_fp16_benchmarking_db.tgz
```
```bash
cd keras_fp16_benchmarking_db
```


**3.1.3** Extract Keras ResNet50 FP32, optimize for inference, and convert to FP16.

Extract Keras ResNet50 FP32 (resnet50_fp32_keras.pb will be generated):

```bash
python gen_resnet50_keras.py
```
Optimize the extracted Keras ResNet50 FP32 graph for inference before casting (resnet50_fp32_keras_opt.pb will be generated):

```bash
python optimize_for_inference.py --graph resnet50_fp32_keras.pb --out_graph resnet50_fp32_keras_opt.pb
```

Convert full graph to FP16 (resnet50_fp16_keras_opt.pb will be generated):
```bash
python fp32tofp16.py  --graph resnet50_fp32_keras_opt.pb --out_graph resnet50_fp16_keras_opt.pb
```

**3.1.4** Compile ResNet50 frozen graph using provided pb2sm_compile.py script on Inf1 instance. This step takes about 4 minutes on Inf1.2xlarge. NOTE: please ensure that the Neuron Compiler is up-to-date by following the setup steps in Lab 1 Section 1.

>We optimized this model with a compiled time batch size of 5. We optimize throughput having a runtime batch size of mutiples of 5. (50 in this case). This step takes about 6 minutes.

```bash
time python pb2sm_compile.py
```
Expect: 
```
INFO:tensorflow:fusing subgraph neuron_op_a73aed4b95ca5d5b with neuron-cc; log file is at /home/ubuntu/keras_fp16_benchmarking_db/compiler_workdir/neuron_op_a73aed4b95ca5d5b/graph_def.neuron-cc.log

INFO:tensorflow:Number of operations in TensorFlow session: 396
INFO:tensorflow:Number of operations after tf.neuron optimizations: 397
INFO:tensorflow:Number of operations placed on Neuron runtime: 395
INFO:tensorflow:Successfully converted rn50_fp16 to rn50_fp16_compiled_batch5/1

real    3m48.434s
user    3m42.063s
sys     0m13.738s
```
At the end of this step, you will see a zipped saved model `rn50_fp16_compiled_batch5.zip` 

```bash
cp rn50_fp16_compiled_batch5.zip ~
```

>Note: If you compiled this on a different host, you will need to copy to your Inf1 instance, Skip for the purpose of this lab: 

```bash

#optional if your compile host is different than deploy host.
scp -i ~/ee-default-keypair.pem ./rn50_fp16_compiled_batch5.zip ubuntu@<instance DNS>:~/ # Ubuntu Image default.
#scp -i ~/ee-default-keypair.pem ./rn50_fp16_compiled_batch5.zip ec2-user@<instance DNS>:~/  # if on AML2  if you are on Amazon 
```

## Lab 3 Section 2: Launch a load test run on Inf1

**3.2.1** Download and unpack the ResNet50 performance package again, this time on Inf1 instance:

```bash
cd ~
wget https://reinventinf1.s3.amazonaws.com/keras_fp16_benchmarking_db.tgz
```
```bash
tar -xzf keras_fp16_benchmarking_db.tgz
```
```bash
cd keras_fp16_benchmarking_db
```

Unzip the saved model into the current directory:

```bash
unzip ~/rn50_fp16_compiled_batch5.zip
```

**3.2.2** Run load test using provided infer_resnet50_keras_loadtest.py script on Inf1 instance (please make sure this is inf1.2xlarge):

> There are total of 4 Neuron Cores on Inf1.2xlarge.  There are 4 sessions of ResNet50 running, each session binds to a Neuron core. There are 4 threads in each of these sessions.  

```bash
time python infer_resnet50_keras_loadtest.py
```
Output:

```
NUM THREADS:  16
NUM_LOOPS_PER_THREAD:  100
USER_BATCH_SIZE:  50
current throughput: 0 images/sec
current throughput: 0 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
...........
current throughput: 2600 images/sec
current throughput: 2400 images/sec
current throughput: 2600 images/sec
current throughput: 2450 images/sec
current throughput: 2550 images/sec
current throughput: 2500 images/sec
current throughput: 2500 images/sec
current throughput: 500 images/sec

real    0m38.645s
user    1m7.550s
sys     0m6.166s
```

NOTE: If you see lower throughput, please make sure that the Inf1 instance is inf1.2xlarge.

**3.2.3** While this is running you can see utilization using neuron-top tool in a separate terminal (it takes about a minute to load; also running neuron-top will lower the throughput to around 1200 images/sec):
```bash
/opt/aws/neuron/bin/neuron-top
```

**Note: Please go back to home directory /home/ubuntu**

```bash
cd ~/
```

[Go To Lab 4](4.%20Profiling%20and%20Debugging.md)

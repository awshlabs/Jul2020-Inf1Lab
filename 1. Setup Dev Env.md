# Reinvent Inf1 Lab: Hands-on Deep Learning Inference with Amazon EC2 Inf1 Instance (20 min)

## Abstract:

In this workshop, you gain hands-on experience with Amazon EC2 Inf1 instances, powered by custom AWS Inferentia chips. Amazon EC2 Inf1 instances offer low-latency, high-throughput, and cost-effective machine learning inference in the cloud. This workshop walks you through taking a trained deep learning model to deployment on Amazon EC2 Inf1 instances by using AWS Neuron, an SDK for optimizing inference using AWS Inferentia processors.  


## Overview:

The Lab has 4 parts, part 1, install the Neuron development environment, custom compile a (pre)-trained model to garget the Inferentia Neuron.  Note this environment can be setup on a C5 instance, direclty on the Inf1 instance which has an Intel Processor, or on any x86 compatible Linux machine.  Part2, 3 deploy the package to model serve and run batch inference on the Inferentia Neuron Processor. Lab 4, Debug and Profile model.

**Note: Please folllow the labs in sequence.**

Lab 1. **Launch** a inf1.6xlarge Instance, **install** the Neuron development environment, Custom compile a pre-trained model to target the Inferentia Neuron Processor.

Lab 2. **install** Neuron run-time and development environment, **test** and **model serve** the compiled ResNet package.

Lab 3. **Compile and Launch** a load test on Inferentia Instance.

Lab 4. **Debug and Profile** your model.


----------

# Lab 1. **Launch** an inf1.6xlarge, **install** the Neuron development environment, Custom compile a pre-trained model to target the Inferentia Neuron Processor.

## Lab 1 Section 1: Launch an EC2 instance to setup Neuron SDK Dev Environment

A typical workflow with the Neuron SDK will be to compile a trained ML model on a compute-optimized compilation server and then the distribute the artifacts to Inf1 instances for execution.  Select an AMI of your choice, which may be Ubuntu 16.x, Ubuntu 18.x, Amazon Linux 2 based. To use a pre-built Deep Learning AMI with Neuron software, see [DLAMI AWS Neuron guide](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia.html).

[Launching a DLAMI Instance with AWS Neuron](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html). 

The curernt version is Deep Learning AMI (Ubuntu **18.04**) Version 28.1


**1.1.0** Setup your SSH env.  For Windows, we recommend installing MobaXterm. https://mobaxterm.mobatek.net/

If you are using AWS Events engine onsite at Re:Invent. Download your SSH key from the events engine dashboard.  https://dashboard.eventengine.run/ Enter your unique events hash, and download the SSH Key to your local laptop/machine.

> On your local machine set SSH configuration to keep the connection live so that the ec2 ssh session does not idle disconnect you.  Add the following configureation to the end of your .ssh/config  

```
ServerAliveInterval 50
```

**1.1.1** Goto the EC2 portal. Choose the **Oregon** Region. Then select, **Deep Learning AMI (Ubuntu 18.04) Version 28.1**  We will run through the installation and update process as an exercise, as we are very actively updating the software tools and packages.
>Please update your system frequently to ensure you are using the latest Neuron packages, this is critical, as the development team is actively updating the compiler to provide better support for the latest Deep learning models and frameworks. 

Use DLAMI: ami-09517d3f1a9a1cd9a 

**1.1.2** Select and start an EC2 instance as your development environment.
It is recommended to use c5.4xlarge or larger, but for the purpose of this lab, we will setup the dev env on the target instance itself, an **inf1.6xlarge** instance.

>inf1.6xlarge (- ECUs, 24 vCPUs, 2.5 GHz, Intel Xeon P-8259L, 48 GiB memory, EBS only)


**1.1.3** Choose an existing ssh key, launch.  Then click on the Connect Button.  Copy the Connecting ssh command to your terminal.

```bash
Example:
ssh -i "[yourec2key].pem" ubuntu@ec2-12-111-130-249.us-west-2.compute.amazonaws.com  
```


**1.1.4** Install virtual environment:
> It is always a best practice to use a virtual environment to ensure you have the best flexibility for package management in working with deep learning frameworks.
```bash
sudo apt-get update
sudo apt-get -y install virtualenv
```

Note: If you see the following errors during apt-get install, please wait a minute or so for background updates to finish and retry apt-get install:

```bash
E: Could not get lock /var/lib/dpkg/lock-frontend - open (11: Resource temporarily unavailable)
E: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), is another process using it?
```

**1.1.5** Setup a new Python 3.6 virtual environment.

```bash
virtualenv --python=python3.6 test_env_p36
source test_env_p36/bin/activate
pip install --no-deps tensorflow_serving_api==1.15
```
> If you are ever disconnected from the instance, make sure you run this command again to get back to the correct virtual environment where you have installed all the correct packages.


```bash
source test_env_p36/bin/activate
```

**1.1.6** Modify Pip configurations to point to the Neuron repository, copy the 4 lines below and paste them together in the terminal.
```bash
tee $VIRTUAL_ENV/pip.conf > /dev/null <<EOF
[global]
extra-index-url = https://pip.repos.neuron.amazonaws.com
EOF
```


**1.1.7** Install the Neuron Compiler software and TensorFlow package with Neuron support.

```bash
pip install neuron-cc tensorflow-neuron
```
Please ignore the following error displayed during installation:
```bash
ERROR: tensorflow-serving-api 1.15.0 requires tensorflow~=1.15.0, which is not installed.
```


## Lab #1 Section 2.  Compile model for deployment onto Inf1 Instance.

In this section we compile the pre-trained Keras ResNet50 model that is included with TensorFlow-Neuron and export it as a SavedModel. SavedModel is an interchange format for TensorFlow models.

>Note that Model size for Resnet is about 20+ million parameters, in FP32 format, each parameter is 4 bytes.  The compiler process automatically converts them into BF16 (2 bytes per parameter), which is a much more efficient dataformat that Neuron Cores support in hardware.

**1.2.1** Create a python script named `compile_resnet50.py` with the following content. You can use either nano (easy) or vi editors.

>Inspect the code below to understand the steps: export savedModel, compile model.

```python
import os
import time
import shutil
import tensorflow as tf
import tensorflow.neuron as tfn
import tensorflow.compat.v1.keras as keras
from tensorflow.keras.applications.resnet50 import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input

# Create a workspace
WORKSPACE = './ws_resnet50'
os.makedirs(WORKSPACE, exist_ok=True)

# Prepare export directory (old one removed)
model_dir = os.path.join(WORKSPACE, 'resnet50')
compiled_model_dir = os.path.join(WORKSPACE, 'resnet50_neuron')
shutil.rmtree(model_dir, ignore_errors=True)
shutil.rmtree(compiled_model_dir, ignore_errors=True)

# Instantiate Keras ResNet50 model
keras.backend.set_learning_phase(0)
tf.keras.backend.set_image_data_format('channels_last')
model = ResNet50(weights='imagenet')

# Export SavedModel
tf.saved_model.simple_save(
    session            = keras.backend.get_session(),
    export_dir         = model_dir,
    inputs             = {'input': model.inputs[0]},
    outputs            = {'output': model.outputs[0]})

# Compile using Neuron
#tfn.saved_model.compile(model_dir, compiled_model_dir) #default compiles to 1 neuron core.
tfn.saved_model.compile(model_dir, compiled_model_dir, compiler_args =['--num-neuroncores', '4']) # compile to 4 neuron cores.

# Prepare SavedModel for uploading to Inf1 instance
shutil.make_archive('./resnet50_neuron', 'zip', WORKSPACE, 'resnet50_neuron')
```

> Note `compiler_args =['--num-neuroncores', '4']`, targets and optimizes the model to run on 4 Neuron cores; default is 1. Be sure to use the correct number of cores on the target Inf1 instance.


**1.2.2** Run the compilation script, which will take a few minutes on c5d.4xlarge. At the end of script execution, the compiled SavedModel is zipped as resnet50_neuron.zip in local directory. This zip file will be needed in the next lab where we deploy the compiled model on an Inf1 instance.

```bash
time python compile_resnet50.py  
```
```
...
INFO:tensorflow:Number of operations in TensorFlow session: 4638
INFO:tensorflow:Number of operations after tf.neuron optimizations: 556
INFO:tensorflow:Number of operations placed on Neuron runtime: 554
INFO:tensorflow:Successfully converted ./ws_resnet50/resnet50 to ./ws_resnet50/resnet50_neuron

real    2m15.616s
user    2m2.873s
sys     0m11.886s
```

>Note: Neuron compiler does ahead-time compilation. Comparing this to JIT, or first time inference compile systems, this **saves** startup time if you are deploying this onto mutliple servers. The Neuron compiler can automatically fuse operator + scheduling and mem management.

**1.2.3** Please keep resnet50_neuron.zip in local directory. You will need this in the next lab. Please click on the link below to go to the next document to launch an Inf1 Instance and deploy your compiled model.

Congratulations! You have completed the first part of this Lab!

[Go To Lab 2](2.%20Deploy%20and%20model%20serve%20on%20Inf1%20Instance.md)
